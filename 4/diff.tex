\section{Differentiation}
Until further notice, we will just study differentiation of a function $f:I\to\mathbb R$ where $I$ is an interval.
\subsection{The Derivative}
\begin{definition}
    If $a\in I$ and we have
    $$\lim_{x\to a}\frac{f(x)-f(a)}{x-a}$$
    exists and equals $c\in\mathbb R$, we say $f$ is differentiable at $a$ and the derivative is $f^\prime(a)=c$.
    Moreover, if $f$ is differentiable at all $a\in I$, we say $f$ is differentiable.
\end{definition}
Geometrically $f^\prime(a)$ is the slope of the tangent line of $f$ at $a$.\\
Another (formal) way to put this is to observe that the definition is essentially $\forall \epsilon>0,\exists\delta>0$,
$$|x-a|<\delta\implies\left|\frac{f(x)-f(a)}{x-a}-c\right|<\epsilon\implies |f(x)-f(a)-c(x-a)|<\epsilon|x-a|$$
Another way we can write is
$$f^\prime(a)=\lim_{h\to 0}\frac{f(a+h)-f(a)}{h}$$
where $h\in I-a$.
\begin{example}
    1. The constant function is differentiable everywhere and has zero derivative everywhere.\\
    2. The identity function is differentiable everywhere and has derivative $1$.\\
    3. $f:(0,\infty)\to\mathbb R$ by $x\mapsto 1/x$ is differentiable and has derivative $f^\prime(a)=-a^{-2}$.\\
    4. (non-example) The function $x\mapsto |x|$ is not differentiable at $0$.
\end{example}
\begin{lemma}
    Suppose $f$ is differentiable at $a$ with derivative $f^\prime(a)$ iff
    $$f(a+h)=f(a)+f^\prime(a)h+\alpha(h)h$$
    where $\alpha(0)=0$ and $\alpha$ is continuous at $0$.
\end{lemma}
Note that given this formula, then $\alpha(h)$ is completely deterined by $f$ and $f^\prime(a)$.
\begin{proof}
    Obvious.
\end{proof}
\begin{corollary}
    $f$ is differentiable at $a$ implies $f$ is continuous at $a$.
\end{corollary}
\begin{proof}
    Follows directly.
\end{proof}
\begin{proposition}
    If $f,g:I\to\mathbb R$ are differentiable at some $a\in I$, so are $f+g,fg$ with derivatives $f^\prime(a)+g^\prime(a),f^\prime(a)g(a)+f(a)g^\prime(a)$ respectively.
\end{proposition}
\begin{proof}
    Trivial.
\end{proof}
\begin{theorem}[Chain Rule]
    Suppose $f:I_1\to I_2$ is differentiable at $a\in I_1$ and $g:I_2\to\mathbb R$ is differentiable at $f(a)$, then $(g\circ f)^\prime(a)=g^\prime(f(a))f^\prime(a)$.
\end{theorem}
\begin{proof}
    Let $b=f(a)$, then by our conditions,
    $$f(a+h)=f(a)+f^\prime(a)h+\alpha(h)h,g(b+k)=g(b)+g^\prime(b)k+\beta(k)k$$
    And $\alpha(0)=\beta(0)=0$ and they are both continuous at $0$.
    Take $k=f(a+h)-f(a)=f^\prime(a)h+\alpha(h)h$, so
    \begin{align*}
        (g\circ f)(a)&=g(b+k)\\
        &=g(b)+g^\prime(b)k+\beta(k)k\\
        &=g(b)+g^\prime(b)(f^\prime(a)h+\alpha(h)h)+\beta(f^\prime(a)h+\alpha(h)h)(f^\prime(a)h+\alpha(h)h)\\
        &=(g\circ f)(a)+g^\prime(f(a))f^\prime(a)h+\phi(h)h
    \end{align*}
    Where $\phi(h)=g^\prime(b)\alpha(h)+\beta(f^\prime(a)h+\alpha(h)h)(f^\prime(a)+\alpha(h))$ which is $0$ at $0$ and is continuous at $0$.
    The chain rule follows.
\end{proof}
\begin{corollary}
    Suppose $f,g:I\to\mathbb R$ are differentiable at $a$ and $g(a)\neq 0$, then $f/g$ is differentiable at $a$ and $(f/g)^\prime=(f^\prime g-fg^\prime)/g^2$.
\end{corollary}
\begin{proof}
    Immediate from the chain rule.
\end{proof}
\begin{corollary}
    The polynomial $p(x)=a_0+a_1x+\cdots+a_nx^n$ is differentiable at all $x\in\mathbb R$, and if $p,q$ are polynomials, $p/q$ is differentiable at $a$ when $q(a)\neq 0$.
\end{corollary}
\begin{proof}
    Trivial.
\end{proof}
\subsection{Mean Value Theorem}
Suppose we have some $f:I\to\mathbb R$.
\begin{definition}
    We say some $c\in I$ is a global maximum of $f$ if $\forall x\in I,f(x)\le f(c)$ and a global minimum if $\forall x\in I,f(x)\ge f(c)$.
\end{definition}
We know that if $I$ is a closed bounded interval, then $f$ has global maxima and minima.
\begin{definition}
    Some $c\in I$ is called a local maximum of $f$ if $\exists\epsilon>0,\forall x\in (c-\epsilon,c+\epsilon)\cap I,f(x)\le f(c)$.
    Similarly it is a local minimum if $\exists\epsilon>0,\forall x\in (c-\epsilon,c+\epsilon)\cap I,f(x)\ge f(c)$.
\end{definition}
\begin{definition}
    $c\in I$ is called an interior point of $I$ if $\exists \epsilon>0,(c-\epsilon,c+\epsilon)\subset I$.
\end{definition}
\begin{proposition}
    Suppose $f:I\to\mathbb R$ and $c$ is an interior point of $I$ where $f$ attains local maximum or local minimum at $c$ and $f$ is differentiable at $c$, then $f^\prime(c)=0$.
\end{proposition}
\begin{proof}
    Suffice to prove the local maximum case.
    Since $c$ is an interior point of $I$, we can choose $\epsilon'>0,(c-\epsilon',c+\epsilon')\subset I$.
    As $c$ is a local maximum, then there is some $\epsilon$ such that $0<\epsilon\le\epsilon'$ and that $\forall x\in (c-\epsilon,c+\epsilon),f(x)\le f(c)$.
    We can easily find sequences $(x_n^-)\in (c-\epsilon,c),(x_n^+)\in (c,c+\epsilon)$ both converging to $c$.
    Let $g:(c-\epsilon,c+\epsilon)\setminus\{c\}\to\mathbb R$ defined by $g(x)=(f(x)-f(c))/(x-c)$, then
    \begin{align*}
        0&\le\lim_{n\to\infty}\frac{f(x_n^-)-f(c)}{x_n^--c}\\
        &=\lim_{x\to c^-}g(x)=\lim_{x\to c}g(x)=\lim_{x\to c^+}g(x)\\
        &=\lim_{n\to\infty}\frac{f(x_n^+)-f(c)}{x_n^+-c} \le 0
    \end{align*}
    Hence $f^\prime(c)=\lim_{x\to c}g(x)=0$.
\end{proof}
\begin{theorem}[Rolle's Theorem]
    Suppose $f:[a,b]\to\mathbb R$ is continuous and is differentiable in $(a,b)$ and $f(a)=f(b)=0$, then there is some $c\in (a,b)$, $f^\prime(c)=0$.
\end{theorem}
\begin{proof}
    It is obvious if $f\equiv 0$, otherwise WLOG $f$ attains some positive value.
    Then let $c$ be a global maximum of $f$, then $c$ is a local maximum and an interior point of $f$ (since $f$ is zero on endpoints but we have assumed that $f$ attains some positive value), then $c\in (a,b)$ and we have $f^\prime(c)=0$ by the preceding proposition.
\end{proof}
\begin{theorem}[Mean Value Theorem]
    If $f:[a,b]\to\mathbb R$ is continuous on $[a,b]$ and differentiable on $(a,b)$, then there is some $c\in (a,b)$ with $f^\prime(c)=(f(b)-f(a))/(b-a)$.
\end{theorem}
\begin{proof}
    Let
    $$p(x)=\frac{1}{b-a}((x-a)f(b)-(x-b)f(a))$$
    which although looks dreadful is just a linear polynomial.
    Note that $p(a)=f(a),p(b)=f(b)$, so it is just a line joining the endpoints.
    Then $g(x)=f(x)-p(x)$ is continuous on $[a,b]$ and differentiable on $(a,b)$, so by Rolle's Theorem, there is some $c\in (a,b)$ sucb that
    $$0=g^\prime(c)=f^\prime(c)-\frac{f(b)-f(a)}{b-a}\implies f^\prime(c)=\frac{f(b)-f(a)}{b-a}$$
    As desired.
\end{proof}
\begin{proposition}
    Suppose $f:[a,b]\to\mathbb R$ is continuous and differentiable in $(a,b)$.\\
    1. $f^\prime\equiv 0\iff f$ is constant.\\
    2. $f^\prime\ge 0\iff f$ is increasing.\\
    3. $f^\prime>0\implies f$ is strictly increasing.
\end{proposition}
\begin{proof}
    Immediate from Mean Value Theorem.
\end{proof}
Note that the converse of the third statement is false in general.
E.g, $f(x)=x^3$.\\
Note that all these depends on the least upper bound property since we used the Mean Value Theorem which depends on Rolle's Theorem which depends on maximum value theorem which depends on Bolzano-Weierstrass Theorem which depends on the Monotone Sequence Theorem which depends on the least-upper-bound property.
\subsubsection{Inverse Function Theorem}
\begin{lemma}
    If $f:[a,b]\to\mathbb R$ satisfies $f^\prime(x)>0$ for all $x\in [a,b]$, then $f:[a,b]\to[f(a),f(b)]$ is a bijection.
\end{lemma}
\begin{proof}
    Injectivity is obvious by Mean Value Theorem.
    For surjectivity we can just exploit Intermediate Value Theorem.
\end{proof}
\begin{lemma}
    Let $I$ be an interval.
    Suppose $f:I\to f(I)$ is a continuous bijection, then $f^{-1}:f(I)\to I$ is also continuous.
\end{lemma}
\begin{proof}
    Just check definition.
\end{proof}
\begin{theorem}[Inverse Function Theorem]
    Suppose $I\subset R$ is an interval and $f^\prime(x)>0$ for any $x\in I$, then the continuous inverse (exists by the preceding lemmas) $f^{-1}:f(I)\to I$ is differentiable and $(f^{-1})^\prime(y)=1/f^\prime(f^{-1}(y))$.
\end{theorem}
\begin{example}
    1. $f:x\mapsto x^n$ on $(0,\infty)$ satisfies the conditions, so its inverse $f^{-1}:y\mapsto y^{1/n}$ has $(f^{-1})^\prime(y)=(1/n)y^{1/n-1}$.\\
    2. Consider $\tan:(-\pi/2,\pi/2)\to\mathbb R$, it shall be invertible with differentiable inverse $\tan^{-1}$, then we can calculate, by the formula, that $(\tan^{-1})(y)=1/(y^2+1)$
\end{example}
\begin{proof}[Proof assuming differentiability]
    Assuming $f^{-1}$ is differentiable, then we know that $f(f^{-1}(y))=y$, then by the chain rule, we get $f^\prime(f^{-1}(y))(f^{-1})^\prime(y)=1$, rearrange to give the formula.
\end{proof}
\begin{proof}[Actual proof]
    Fix $b\in f(I)$ and let $a=f^{-1}(b)$.
    Suppose $(y_n)$ is a sequence in $f(I)\setminus \{b\}$ with $y_n\to b$, and let $x_n=f^{-1}(y_n)$.
    We know that $f$ is differentiable hence continuous, so $f^{-1}$ is continuous by the preceding lemma.
    Therefore $x_n=f^{-1}(y_n)\to f^{-1}(b)=a$.
    Since we know that
    $$\lim_{x\to a}\frac{f(x)-f(a)}{x-a}=f^\prime(a)$$
    We have
    $$\frac{f(x_n)-f(a)}{x_n-a}\to f^\prime(a)$$
    By hypothesis, $f^\prime(a)\neq 0$, so
    $$\frac{f^{-1}(y_n)-f^{-1}(b)}{y_n-b}=\frac{x_n-a}{f(x_n)-f(a)}\to\frac{1}{f^\prime(a)}=\frac{1}{f^\prime(f^{-1}(b))}$$
    which is true for every sequence $y_n\to b$, so the theorem is proved.
\end{proof}
\subsubsection{L'H\^opital's Rule}
\begin{proposition}[L'H\^opital's Rule]
    Suppose $f,g:I\to\mathbb R$ are differentiable with $f(a)=g(a)=0$, and $\exists r>0, g(x)\neq 0\neq g^\prime(x)$ for $0<|x-a|<r$.
    Then if $\lim_{x\to a}f^\prime(a)/g^\prime(a)$ exists and equals $k$, then
    $$\lim_{x\to a}\frac{f(x)}{g(x)}=k$$
\end{proposition}
\begin{proof}
    Suppose $x\in I$ and $x>a$.
    Consider $h(t)=f(t)g(x)-f(x)g(t),t\in [a,x]$.
    Since $f,g$ are differentiable and we have $h(a)=h(a)=0$, so by Rolle's Theorem, there is some $c\in (a,x)$ with $f^\prime(c)g(x)-f(x)g^\prime(c)=h^\prime(c)=0$.
    Therefore
    $$\frac{f^\prime(c)}{g^\prime(c)}=\frac{f(x)}{g(x)}$$
    Same for $x<a$.\\
    Given $\epsilon>0$, choose $\delta>0$ such that
    $$0<|c-a|<\delta\implies\left|\frac{f^\prime(c)}{g^\prime(c)}-k\right|<\epsilon$$
    So when $0<|x-a|<\delta$, one can find $c$ such that $0<|c-a|<|x-a|<\delta$ and $f^\prime(c)/g^\prime(c)=f(x)/g(x)$, which means that
    $$\left|\frac{f(x)}{g(x)}-k\right|=\left|\frac{f^\prime(c)}{g^\prime(c)}-k\right|<\epsilon$$
    which is as we wanted.
\end{proof}
\subsection{Higher Derivatives and Taylor Series}
\begin{definition}
    We say (inductively) that $f:I\to\mathbb R$ is $k$ times differentiable with $k^{th}$ derivative $f^{(k)}$ if $f$ is $(k-1)$ times differentiable with differentiable $(k-1)^{th}$ derivative $f^{(k-1)}$ and $f^{(k-1)})^\prime=f^{(k)}$.
\end{definition}
\begin{example}
    A polynomial is $k$ times differentiable for any $k\in\mathbb N$.
\end{example}
\begin{definition}
    We say $f$ is $C^k$ if $f$ is $k$ times differentiable and $f^{(k)}$ is continuous.
\end{definition}
\begin{example}[Non-example]
    Consider
    $$f(x)=\begin{cases}
        x^2\sin(1/x)\text{, for $x\neq 0$}\\
        0\text{, for $x=0$}
    \end{cases}$$
    So one can verify that $f$ is differentiable but the derivative, namely,
    $$f^\prime(x)=\begin{cases}
        2x\sin(1/x)-\cos(1/x)\text{, for $x\neq 0$}\\
        0\text{, for $x=0$}
    \end{cases}$$
    is not continuous at $0$.
\end{example}
Now, given $f:I\to\mathbb R$, we want a degree $k$ polynomial that best approximates $f$.
The answer depends on the meaning of ``best''.
We can either calculate the total squared difference (we have not defined it and will not care about it), but on the other hand, the theory of differentiation hints that
$$f(x)=f(a)+f^\prime(a)(x-a)=P_1(x)$$
is the best linear approximation of $f$ around $a$.
Why ``around $a$''?
say for example we want to approximate $\sin x$ over a large interval, then the best approximation intuitively might be a line with slope very close to $0$, which is obviously not the tangent at, e.g., $0$.
And we are only interested in the best approximation close to $a$.\\
Now back to our best linear approximation.
The reason we say it is the ``best'' in the sense that $f(x)-P_1(h)=\alpha(h)h$ with $\alpha(0)=0$ and $\alpha$ is continuous at $0$.
We can also easily know that such a $P_1$ is unique so as for such an $\alpha$ to exist by working out their coefficients by substitution and differentiating (with appropriate comments on differentiability, of course).
More generally, for a sufficiently smooth $f$, we can try to approximate $f(a+h)=P_k(a+h)+h^k\alpha(h)$ where $\alpha(0)=0$ and $\alpha$ is continuous at $0$ and $P_k$ is a polynomial.
Using the same trick, such an $P_k$ is unique.
In particular, we obtain $P_k^{(r)}=f^{(r)}$ for any $r\le k$.
\begin{lemma}
    If $a,c_0,c_1,\ldots,c_k\in\mathbb R$, then there is a unique degree $k$ polynomial $P$ with $P^{(i)}(a)=c_i$.
\end{lemma}
\begin{proof}
    Write out $P(x)=a_0+a_1(x-a)+\cdots+a_k(x-a)^k$ and differentiating gives $i!a_i=c_i=P^{(i)}(a)$, so the polynomial is uniquely determined.
\end{proof}
\begin{definition}
    Given $f:I\to\mathbb R$ which is $k$ times differentiable and $a$ is an interior point of $I$, then the $k^{th}$ Taylor polynomial of $f$ centered at $a$ is
    $$P_k(x)=\sum_{i=0}^k\frac{f^{(i)}(a)}{i!}(x-a)^i$$
    which is chosen such that $P_k^{(i)}(a)=f^{(i)}(a)$.
\end{definition}
\begin{remark}
    $P_k(x)$ is the unique polynomial with degree at most $k$ having the property $P_k^{(i)}(a)=f^{(i)}(a)$ by the preceding lemma.
\end{remark}
\begin{theorem}[Taylor's Theorem with Remainder]
    Suppose $f:I\to\mathbb R$ is $(k+1)$ times differentiable, and $[a,x]\subset I$, then
    $$f(x)=\left(\sum_{i=0}^k\frac{f^{(i)}(a)}{i!}(x-a)^i\right)+\frac{f^{(k+1)}(c)}{(k+1)!}(x-a)^{k+1}$$
    for some $c\in (a,x)$.
\end{theorem}
For $k=0$, this reduces to the mean value theorem.
\begin{proof}
    Fix $x>a$ and define
    $$g(t)=f(t)+\sum_{i=1}^k\frac{f^{(i)}(t)}{i!}(x-t)^i+\alpha\frac{(x-t)^{k+1}}{(k+1)!}$$
    where $\alpha$ is chosen such that $g(a)=f(x)$ (which exists as a solution to a nondegenerate linear equation).
    Our goal is to find $c\in (a,x)$ such that $f^{(k+1)}(c)=\alpha$.
    We have, by definition,
    $$f(x)=g(a)=f(a)+\sum_{i=1}^k\frac{f^{(i)}(a)}{i!}(x-a)^i+\alpha\frac{(x-a)^{k+1}}{(k+1)!}$$
    Now $f$ is $k+1$ times differentiable so $g$ is differentiable and $g(a)=f(x)=g(x)$, so by Rolle's Theorem, there is some $c\in (a,x)$ such that $g^\prime(c)=0$.
    But we have
    $$g^\prime(t)=f^\prime(t)+\sum_{i=1}^k\left(\frac{f^{(i+1)}(t)}{i!}(x-t)^i-\frac{f^{(i)}(t)}{(i-1)!}(x-t)^{i-1}\right)-\alpha\frac{(x-t)^k}{k!}$$
    The first part of this expression is telescoping, so we can simplify it to
    $$g^\prime(t)=(f^{(k+1)}(t)-\alpha)\frac{(x-t)^k}{k!}$$
    Now plugging in $t=c$ shows $f^{(k+1)}(c)=\alpha$, which completes the proof.
\end{proof}
If $f$ is infinitely differentiable, one can then form the Taylor series
$$\sum_{i=0}^\infty\frac{f^{(i)}(a)}{i!}(x-a)^i$$
as the formal limit of the Taylor polynomilas $P_k(x)$, which sometimes does converge and equals $f$.
If we understand the derivatives of $f$ well-enough and that those derivatives do behave nicely, then it is sometimes possible to use Taylor's Theorem to prove that
$$p(x)=\lim_{n\to\infty}P_n(x)=f(x)$$
\begin{example}
    Take $f(x)=\cos x$ (which, albeit hasn't been properly defined, we shall assume standard facts about), then the Taylor series about $0$ is
    $$1-\frac{x^2}{2!}+\frac{x^4}{4!}-\frac{x^6}{6!}+\cdots=\sum_{n=0}^\infty(-1)^n\frac{x^{2n}}{(2n)!}$$
    The series always converges by the ratio test.
    Must the series converges to $\cos x$ then?
    The $(2k)^{th}$ partial sum of the series is exactly $P_{2k}(x)$ of $\cos$ about $0$, then there is $c\in (0,x)$ with
    $$|f(x)-P_{2k}(x)|=\left|\frac{f^{2k+1}(c)}{(2k+1)!}x^{2k+1}\right|\le\frac{|x|^{2k+1}}{(2k+1)!}\to 0$$
    as $k\to\infty$, so the series does converge to $\cos x$.
\end{example}
\begin{definition}
    We say $f:I\to\mathbb R$ is $C^\infty$ (or smooth) if $f$ is $C^k$ for all $k\in\mathbb N$.
\end{definition}
So for a smooth $f:I\to\mathbb R$, one can form the Taylor series (formally)
$$\sum_{n=0}^\infty\frac{f^{(n)}(a)}{n!}(x-a)^n$$
But even if the series converges for all $x\in I$, it may not converge to $f(x)$ for $x\neq a$.
Philosophically, one cannot fully expect that the behaviour of $f$ near $a$ control its global behaviour.
\footnote{For complex-valued functions, however, the local behaviour of a nice enough function $f$ can be controlled by its local properties.}
Indeed, for $f_1,f_2:\mathbb R\to\mathbb R$ smooth and $a<b$, it is possible to construct some smooth $f:\mathbb R\to\mathbb R$ such that $f|_{<a}=f_1|_{<a}$ and $f|_{>b}=f_2|_{>b}$.
This will be proven in example sheet.
\subsection{Complex Differentiation}
\begin{definition}
    If $c\in\mathbb C,r\in[0,\infty)$, we define the open ball of radius $r$ centered at $c$ by $B_r(c)=\{z\in\mathbb C:|z-c|<r\}$.
\end{definition}
\begin{definition}
    A subset $\Omega\subset\mathbb C$ is open if $\forall c\in\Omega,\exists\epsilon>0,B_\epsilon(c)\subset\Omega$.
\end{definition}
\begin{example}
    1. $\mathbb C,\varnothing$ are open.\\
    2. Open balls are open.\\
    3. (non-example) $\{z\in\mathbb C:|z-a|\le r\}$ is not open, but its complement is.
\end{example}
\begin{definition}
    Suppose $\Omega\in\mathbb C$ is open and $c\in\Omega$.
    For a function $f:\Omega\to\mathbb C$, we say $f$ is complex differentiable at $c$ if
    $$\lim_{z\to c}\frac{f(z)-f(c)}{z-c}$$
    exists.
    In case it does exist, we write it as $f^\prime(c)$.
\end{definition}
All the rules in real differentiation still applies, but we don't really have mean value theorem.
We can write $f(x+iy)=u(x,y)+iv(x,y)$ where $u,v:\tilde{\Omega}\to\mathbb R$ where $\tilde{\Omega}=\{(x,y)\in\mathbb R^2:x+iy\in\Omega\}$.
\begin{definition}
    Say $\tilde{\Omega}$ is open if $\Omega$ is.
    Then the partial derivatives of $F:\tilde{\Omega}\to\mathbb R$ at $(a,b)\in\tilde{\Omega}$ are
    \begin{align*}
        \left.\frac{\partial F}{\partial x}\right|_{(a,b)}&=\lim_{h\to 0}\frac{F(a+h,b)-F(a,b)}{h}\\
        \left.\frac{\partial F}{\partial y}\right|_{(a,b)}&=\lim_{h\to 0}\frac{F(a,b+h)-F(a,b)}{h}
    \end{align*}
    Provided that they exist.
\end{definition}
Alternatively we can define $g_b=F(x,b)$ and so
$$\left.\frac{\partial F}{\partial x}\right|_{(a,b)}=g^\prime_b(a)$$
Similar for the other component.
\begin{proposition}
    If $f:\Omega\to\mathbb C$ is complex differentiable at $c=a+ib$ with derivative $f^\prime(c)=\alpha+i\beta$, then the partial derivatives of $u,v$ at $(a,b)$ exist and satisfy
    \begin{align*}
        \left.\frac{\partial u}{\partial x}\right|_{(a,b}&= \left.\frac{\partial v}{\partial y}\right|_{(a,b)}=\alpha\\
        -\left.\frac{\partial u}{\partial y}\right|_{(a,b}&=\left.\frac{\partial v}{\partial x}\right|_{(a,b)}=\beta
    \end{align*}
\end{proposition}
\begin{proof}
    For any sequence $h_n\to 0$ in $\mathbb R\setminus\{0\}$, then it is also a sequence in $\mathbb C\setminus\{0\}$, so since $f$ is differentiable at $c$,
    \begin{align*}
        \alpha+i\beta&=f^\prime(c)\\
        &=\lim_{h\to 0}\frac{f(c+h)-f(c)}{h}\\
        &=\lim_{n\to\infty}\frac{u(a+h_n,b)-u(a,b)}{h_n}+i\lim_{n\to\infty}\frac{v(a+h_n,b)-v(a,b)}{h_n}
    \end{align*}
    So $\partial u/\partial x$ and $\partial v/\partial x$ exist and are $\alpha,\beta$ respectively.
    We can get the other half of the equations by replacing $h_n$ by $ih_n$ and casting exactly the same argument.
\end{proof}
\begin{remark}
    If $f:\Omega\to\mathbb C$ is complex differentiable, then (assuming $C^1$ partial derivatives and symmetry of partial derivatives)
    $$\frac{\partial^2u}{\partial x^2}+\frac{\partial^2u}{\partial y^2}=0$$
    which is the Laplace Equation.
\end{remark}
\begin{proposition}
    Suppose $F:B_r((a,b))\to\mathbb R$ has partial derivatives with
    $$\frac{\partial F}{\partial x}=\frac{\partial F}{\partial y}=0$$
    then $F$ is constant.
\end{proposition}
\begin{proof}
    For $(a',b')\in B_r((a,b))$, we consider the functions $f_b(x)=F(x,b)$ and $f_a(x)=F(a',x)$ on the reals.
    Then they are differentiable and using mean value theorem on $f_b$ wrt $a,a'$ shows $f(a,b)=f(a',b)$, and using it again on $f_a$ wrt $b,b'$ shows $f(a',b)=f(a',b')$, so $f(a,b)=f(a',b')$.
\end{proof}
\begin{corollary}
    If $f:B_r(c)\to\mathbb C$ has derivative zero, then $f$ is constant.
\end{corollary}
\begin{proof}
    Immediate.
\end{proof}
\begin{remark}
    Complex differentiable functions $f:\mathbb C\to\mathbb C$ behave very nicely.
    Firstly, if $f$ is complex differentiable, so is $f^\prime$, consequently $f$ is $C^\infty$.
    Even better, its Taylor series always converges to itself.
    Secondly, if $f$ is bounded then $f$ is constant.
\end{remark}